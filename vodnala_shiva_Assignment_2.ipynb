{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vodnalashiva131/INFO-5731/blob/main/vodnala_shiva_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDyTKYs-yGit",
        "outputId": "04301e46-0790-4f59-c725-4b1d57cceced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will retrieve up to 10,000 documents out of an estimated 654257 documents\n",
            "Retrieved 1000 papers...\n",
            "Retrieved 2000 papers...\n",
            "Retrieved 3000 papers...\n",
            "Retrieved 4000 papers...\n",
            "Retrieved 4999 papers...\n",
            "Retrieved 5998 papers...\n",
            "Retrieved 6998 papers...\n",
            "Retrieved 7998 papers...\n",
            "Retrieved 8998 papers...\n",
            "Retrieved 9998 papers...\n",
            "Retrieved 10000 papers...\n",
            "Done! Retrieved 10000 papers total.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "stored_proc = \"data science\"\n",
        "attributes = \"title,year,abstract\"\n",
        "\n",
        "\n",
        "web_url = f\"http://api.semanticscholar.org/graph/v1/paper/search/bulk?query={stored_proc}&fields={attributes}\"\n",
        "url = requests.get(web_url).json()\n",
        "\n",
        "if 'total' in url:\n",
        "    print(f\"Will retrieve up to 10,000 documents out of an estimated {r['total']} documents\")\n",
        "else:\n",
        "    print(\"Retrieving process of the documents is failed\")\n",
        "    exit()\n",
        "\n",
        "retrived_documents = 0\n",
        "retriveing_limits = 10000\n",
        "\n",
        "with open(\"retrived_papers.csv\", \"w\", newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Title\", \"Year\", \"Abstract\"])\n",
        "\n",
        "    while retrived_documents < retriveing_limits:\n",
        "        if \"data\" in url:\n",
        "            for paper in url[\"data\"]:\n",
        "                writer.writerow([paper.get(\"title\", \"N/A\"), paper.get(\"year\", \"N/A\"), paper.get(\"abstract\", \"N/A\")])\n",
        "                retrived_documents += 1\n",
        "                if retrived_documents >= retriveing_limits:\n",
        "                    break\n",
        "            print(f\"Retrieved {retrived_documents} papers...\")\n",
        "        else:\n",
        "            print(\"Error while retrieving data\")\n",
        "            break\n",
        "\n",
        "        if \"token\" in r and retrived_documents < retriveing_limits:\n",
        "            r = requests.get(f\"{url}&token={r['token']}\").json()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "print(f\"Done! Retrieved {retrived_documents} papers total.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqx2mzjMHOV1",
        "outputId": "7e4981d3-f33d-4dfd-dd4a-223fdfab23d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv(\"retrived_papers.csv\")\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "df['Clean_Abstract'] = df['Abstract'].apply(clean_text)\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_and_lemmatize(text):\n",
        "    try:\n",
        "        text = ' '.join([porter_stemmer.stem(word) for word in text.split()])\n",
        "        text = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    except Exception as e:\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "df['Clean_Abstract'] = df['Clean_Abstract'].apply(stem_and_lemmatize)\n",
        "\n",
        "df.to_csv(\"cleaned_cleared_papers.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-J1Z2O3I9ff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "clean_text = df['Clean_Abstract'].dropna().tolist()\n",
        "\n",
        "parts_of_speech = []\n",
        "named_entitity_recognition = []\n",
        "dependent_outputs = []\n",
        "\n",
        "for sentence in clean_text:\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    parts_of_speech_tags = pos_tag(tokens)\n",
        "    parts_of_speech.extend(parts_of_speech_tags)\n",
        "\n",
        "    chunked = ne_chunk(parts_of_speech_tags)\n",
        "    named_entitity_recognition.extend(chunked)\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "    dependent_outputs.extend([(token.text, token.dep_, token.head.text) for token in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyz0mjT-RprE",
        "outputId": "8bb98879-ede5-45f4-dc0d-2311013ed846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS Counts: NN      1073592\n",
            "JJ       322449\n",
            "VBP       72782\n",
            "NNS       46660\n",
            "IN        32279\n",
            "RB        27210\n",
            "VB        25230\n",
            "VBD       24762\n",
            "FW        16811\n",
            "VBZ       13623\n",
            "CD        12399\n",
            "NNP        9638\n",
            "VBN        8993\n",
            "MD         7171\n",
            "JJR        5059\n",
            "JJS        4222\n",
            "RBR        1739\n",
            "CC         1564\n",
            "DT          866\n",
            "VBG         731\n",
            "RP          324\n",
            "WP          233\n",
            "RBS         230\n",
            "PRP         210\n",
            "WP$         200\n",
            "WRB         142\n",
            "TO           95\n",
            "PRP$         51\n",
            "WDT          44\n",
            "SYM          26\n",
            "POS          23\n",
            "EX           11\n",
            "UH           10\n",
            "NNPS          5\n",
            "PDT           3\n",
            "''            2\n",
            "Name: POS_Tag, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "pos_df = pd.DataFrame(parts_of_speech, columns=['Word', 'POS_Tag'])\n",
        "dependency_df = pd.DataFrame(dependent_outputs, columns=['Word', 'Dependency', 'Head'])\n",
        "\n",
        "print(\"POS\", pos_df['POS_TAGS'].value_counts())\n",
        "\n",
        "pos_df.to_csv(\"total_pos_results.csv\", index=False)\n",
        "dependency_df.to_csv(\"dependencies_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fDYbU4qPxyg"
      },
      "outputs": [],
      "source": [
        "\"\"\"The given task was very good but a little difficult. i was facing difficulty while gathering and cleansing heavy components of data.almost 10,000 research papers abstracts from\n",
        "semantic scholar and cleaning the texts data by removing stopwords,numeric's, and also noise in the data and conducting stemming and lemmatization.In this kind of assignments\n",
        "i got a chance to work on NLP wwhich helpmed to learn new concepts in these assignment. this approaches included name entity identification, constituency parsing ,\n",
        "POS tagging and other dependency parsing.this methods are used to extract relevant information.this assignment is done by dividing the assignment in to smaller jobs and done the work.\n",
        "i have used many various libraries such as numpy,spacy,pandas for retriving data.and process them. this assignment helped me to gain skills that are i am not expertise of.\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "07bf2e0fcd790eb3c900db4935f9ba6ea43c1c1bb69b00af6a6283ca1c5858b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}