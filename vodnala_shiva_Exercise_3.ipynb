{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vodnalashiva131/INFO-5731/blob/main/vodnala_shiva_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQBLtmwjViqT"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "It looks as if you've supplied a complete assessment of diverse functions that can be used for sentiment analysis of product reviews. here's a breakdown of the way every feature contributes to sentiment analysis:\n",
        "\n",
        "Bag-of-Words: BoW representation captures the presence and frequency of phrases in evaluations. it's a easy but powerful way to symbolize textual content facts, allowing models to analyze from the incidence of phrases associated with tremendous, negative, or neutral sentiments.\n",
        "\n",
        "TF-IDF: TF-IDF allows to downweight the importance of words which can be not unusual across files however less informative approximately sentiment. it is able to assist distinguish between phrases which can be common however no longer always indicative of sentiment.\n",
        "\n",
        "N-grams: N-grams capture contextual data by way of considering sequences of words in preference to individual words. This lets in fashions to learn from terms or expressions that bring sentiment extra successfully, which include \"no longer exact\" or \"very glad.\"\n",
        "\n",
        "Sentiment Lexicons: Sentiment lexicons provide lists of phrases with their related sentiment polarity. through incorporating those lexicons, fashions can extract features indicating the presence of sentiment-bearing words, which enables in accurately classifying reviews primarily based on sentiment.\n",
        "\n",
        "Part-of-Speech Tags: POS tagging allows capture syntactic systems and grammatical styles that make contributions to sentiment. via identifying adjectives, adverbs, and different parts of speech that frequently carry sentiment facts, models can higher recognize the sentiment expressed in critiques.\n",
        "\n",
        "Every of these features gives unique insights into the sentiment of product critiques, and combining them can provide a richer illustration of the text statistics, leading to extra correct sentiment type. system mastering models like SVM, Naive Bayes, or neural networks can leverage these capabilities to efficaciously classify critiques into tremendous, poor, or impartial classes. Experimenting with unique combos of features and fashions can in addition beautify the accuracy of sentiment analysis for product opinions.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoQX5s4O70nf",
        "outputId": "4e67354f-34df-49a6-c7db-56804e3980cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Bag-of-Words Features:\n",
            "[[0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0]\n",
            " [0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 2 0 2 0 0]]\n",
            "['am' 'and' 'customer' 'damaged' 'delivery' 'disappointing' 'exceeded'\n",
            " 'excellent' 'expectations' 'experience' 'good' 'great' 'is' 'it' 'late'\n",
            " 'my' 'not' 'of' 'packaging' 'performance' 'poor' 'product' 'quality'\n",
            " 'recommend' 'satisfied' 'service' 'the' 'this' 'was' 'with' 'would']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import opinion_lexicon\n",
        "\n",
        "comments = [\n",
        "    \"This product is great! It exceeded my expectations.\",\n",
        "    \"The product quality is poor and disappointing.\",\n",
        "    \"I am satisfied with the performance of this product.\",\n",
        "    \"Not a good experience. I would not recommend it.\",\n",
        "    \"The customer service was excellent.\",\n",
        "    \"The delivery was late and the packaging was damaged.\"\n",
        "]\n",
        "\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_features = bow_vectorizer.fit_transform(comments)\n",
        "print(\"Bag-of-Words Features:\")\n",
        "print(bow_features.toarray())\n",
        "print(bow_vectorizer.get_feature_names_out())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R5u7Jm0VMHK",
        "outputId": "414b6119-fae7-4508-8412-3e8b87f907eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Features:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.39233589 0.         0.39233589 0.         0.         0.39233589\n",
            "  0.32172105 0.32172105 0.         0.39233589 0.         0.\n",
            "  0.         0.         0.         0.27161901 0.         0.\n",
            "  0.         0.         0.         0.32172105 0.         0.\n",
            "  0.        ]\n",
            " [0.         0.36042932 0.         0.         0.         0.43954028\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.36042932 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.43954028 0.30429919 0.43954028 0.\n",
            "  0.         0.         0.26076129 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.3921214  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.3921214\n",
            "  0.         0.3921214  0.         0.27147051 0.         0.\n",
            "  0.3921214  0.         0.2326296  0.32154515 0.         0.3921214\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.33957035 0.33957035 0.\n",
            "  0.         0.27845254 0.         0.         0.6791407  0.\n",
            "  0.         0.         0.         0.         0.         0.33957035\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.33957035]\n",
            " [0.         0.         0.49848319 0.         0.         0.\n",
            "  0.         0.49848319 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.49848319 0.29572971 0.         0.40876335 0.\n",
            "  0.        ]\n",
            " [0.         0.27690006 0.         0.3376771  0.3376771  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.3376771  0.         0.         0.\n",
            "  0.3376771  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.40066006 0.         0.55380011 0.\n",
            "  0.        ]]\n",
            "['am' 'and' 'customer' 'damaged' 'delivery' 'disappointing' 'exceeded'\n",
            " 'excellent' 'expectations' 'experience' 'good' 'great' 'is' 'it' 'late'\n",
            " 'my' 'not' 'of' 'packaging' 'performance' 'poor' 'product' 'quality'\n",
            " 'recommend' 'satisfied' 'service' 'the' 'this' 'was' 'with' 'would']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "tfidf_features = tfidf.fit_transform(comments)\n",
        "print(\"TF-IDF Features:\")\n",
        "print(tfidf_features.toarray())\n",
        "print(tfidf.get_feature_names_out())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7cww7HGVMHL",
        "outputId": "f9de8d37-d663-4436-89e0-6c82a3e2f963"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N-gram Features:\n",
            "[[('This', 'product'), ('product', 'is'), ('is', 'great'), ('great', '!'), ('!', 'It'), ('It', 'exceeded'), ('exceeded', 'my'), ('my', 'expectations'), ('expectations', '.')], [('The', 'product'), ('product', 'quality'), ('quality', 'is'), ('is', 'poor'), ('poor', 'and'), ('and', 'disappointing'), ('disappointing', '.')], [('I', 'am'), ('am', 'satisfied'), ('satisfied', 'with'), ('with', 'the'), ('the', 'performance'), ('performance', 'of'), ('of', 'this'), ('this', 'product'), ('product', '.')], [('Not', 'a'), ('a', 'good'), ('good', 'experience'), ('experience', '.'), ('.', 'I'), ('I', 'would'), ('would', 'not'), ('not', 'recommend'), ('recommend', 'it'), ('it', '.')], [('The', 'customer'), ('customer', 'service'), ('service', 'was'), ('was', 'excellent'), ('excellent', '.')], [('The', 'delivery'), ('delivery', 'was'), ('was', 'late'), ('late', 'and'), ('and', 'the'), ('the', 'packaging'), ('packaging', 'was'), ('was', 'damaged'), ('damaged', '.')]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "n = 2\n",
        "ngram_features = []\n",
        "for comment in comments:\n",
        "    tokens = word_tokenize(comment)\n",
        "    bigrams = list(ngrams(tokens, n))\n",
        "    ngram_features.append(bigrams)\n",
        "print(\"N-gram Features:\")\n",
        "print(ngram_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaD3cOx2VMHM",
        "outputId": "ed40f34b-696b-4dba-cf87-596dcce12cbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Lexicon Features:\n",
            "[['great', 'exceeded'], ['poor', 'disappointing'], ['satisfied'], ['good', 'recommend'], ['excellent'], ['damaged']]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "sentiment_lexicon = opinion_lexicon.words()\n",
        "sentiment_features = []\n",
        "for comment in comments:\n",
        "    tokens = word_tokenize(comment.lower())\n",
        "    sentiment_words = [word for word in tokens if word in sentiment_lexicon]\n",
        "    sentiment_features.append(sentiment_words)\n",
        "print(\"Sentiment Lexicon Features:\")\n",
        "print(sentiment_features)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM4L-080VMHO",
        "outputId": "a25fa260-71c7-4b2b-86d9-426850f14b13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POS Features:\n",
            "[[('This', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('!', '.'), ('It', 'PRP'), ('exceeded', 'VBD'), ('my', 'PRP$'), ('expectations', 'NNS'), ('.', '.')], [('The', 'DT'), ('product', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('poor', 'JJ'), ('and', 'CC'), ('disappointing', 'JJ'), ('.', '.')], [('I', 'PRP'), ('am', 'VBP'), ('satisfied', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('performance', 'NN'), ('of', 'IN'), ('this', 'DT'), ('product', 'NN'), ('.', '.')], [('Not', 'RB'), ('a', 'DT'), ('good', 'JJ'), ('experience', 'NN'), ('.', '.'), ('I', 'PRP'), ('would', 'MD'), ('not', 'RB'), ('recommend', 'VB'), ('it', 'PRP'), ('.', '.')], [('The', 'DT'), ('customer', 'NN'), ('service', 'NN'), ('was', 'VBD'), ('excellent', 'JJ'), ('.', '.')], [('The', 'DT'), ('delivery', 'NN'), ('was', 'VBD'), ('late', 'JJ'), ('and', 'CC'), ('the', 'DT'), ('packaging', 'NN'), ('was', 'VBD'), ('damaged', 'VBN'), ('.', '.')]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pos_features = []\n",
        "for revcommentiew in comments:\n",
        "    tokens = word_tokenize(comment)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_features.append(pos_tags)\n",
        "print(\"POS Features:\")\n",
        "print(pos_features)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CRuXfV570ng",
        "outputId": "9da4cf83-0546-43c6-c139-e55347e80441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ranked Features based on Mutual Information:\n",
            "Feature: and, Importance: 0.5194444444444445\n",
            "Feature: experience, Importance: 0.3388888888888889\n",
            "Feature: of, Importance: 0.2416666666666667\n",
            "Feature: poor, Importance: 0.2416666666666667\n",
            "Feature: packaging, Importance: 0.19999999999999996\n",
            "Feature: this, Importance: 0.18611111111111112\n",
            "Feature: great, Importance: 0.15833333333333344\n",
            "Feature: expectations, Importance: 0.07499999999999996\n",
            "Feature: quality, Importance: 0.07499999999999996\n",
            "Feature: recommend, Importance: 0.03333333333333344\n",
            "Feature: satisfied, Importance: 0.019444444444444597\n",
            "Feature: am, Importance: 0.0\n",
            "Feature: customer, Importance: 0.0\n",
            "Feature: damaged, Importance: 0.0\n",
            "Feature: delivery, Importance: 0.0\n",
            "Feature: disappointing, Importance: 0.0\n",
            "Feature: exceeded, Importance: 0.0\n",
            "Feature: excellent, Importance: 0.0\n",
            "Feature: good, Importance: 0.0\n",
            "Feature: is, Importance: 0.0\n",
            "Feature: it, Importance: 0.0\n",
            "Feature: late, Importance: 0.0\n",
            "Feature: my, Importance: 0.0\n",
            "Feature: not, Importance: 0.0\n",
            "Feature: performance, Importance: 0.0\n",
            "Feature: product, Importance: 0.0\n",
            "Feature: service, Importance: 0.0\n",
            "Feature: the, Importance: 0.0\n",
            "Feature: was, Importance: 0.0\n",
            "Feature: with, Importance: 0.0\n",
            "Feature: would, Importance: 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "all_features = bow_features.toarray()\n",
        "labels = [1, 0, 1, 0, 1, 0]\n",
        "matched_information = matched_information_classif(all_features, labels)\n",
        "feature_importance = list(zip(bow_vectorizer.get_feature_names_out(), matched_information))\n",
        "sorted_features = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
        "print(\"Ranked Features based on Mutual Information:\")\n",
        "for feature, importance in sorted_features:\n",
        "    print(f\"Feature: {feature}, Importance: {importance}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPJsTA_lVMHS",
        "outputId": "1bdd0a4a-a461-4b88-a97c-afbc81332580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Ranked Text Documents based on Similarity to Query:\n",
            "Similarity: 0.8104807138442993, Review: I am satisfied with the performance of this product.\n",
            "Similarity: 0.8018361330032349, Review: The customer service was excellent.\n",
            "Similarity: 0.7438066005706787, Review: This product is great! It exceeded my expectations.\n",
            "Similarity: 0.26841312646865845, Review: The product quality is poor and disappointing.\n",
            "Similarity: 0.24808931350708008, Review: The delivery was late and the packaging was damaged.\n",
            "Similarity: 0.24685847759246826, Review: Not a good experience. I would not recommend it.\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "comments = [\n",
        "    \"This product is great! It exceeded my expectations.\",\n",
        "    \"The product quality is poor and disappointing.\",\n",
        "    \"I am satisfied with the performance of this product.\",\n",
        "    \"Not a good experience. I would not recommend it.\",\n",
        "    \"The customer service was excellent.\",\n",
        "    \"The delivery was late and the packaging was damaged.\"\n",
        "]\n",
        "\n",
        "query = \"I want to buy a product with excellent quality and good customer service.\"\n",
        "\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "q_embedded = model.encode(query, convert_to_tensor=True)\n",
        "reviewed_embiddings = model.encode(comments, convert_to_tensor=True)\n",
        "q_embeddeds = q_embedded.reshape(1, -1)\n",
        "reviewed_embiddings = reviewed_embiddings.reshape(len(comments), -1)\n",
        "similarities = cosine_similarity(q_embedded, reviewed_embiddings)\n",
        "ranked_indexes = np.argsort(similarities[0])[::-1]\n",
        "print(\"Ranked Text Documents based on Similarity to Query:\")\n",
        "for idx in ranked_indexes:\n",
        "    print(f\"Similarity: {similarities[0][idx]}, Review: {comments[idx]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4BVq_IXY__5"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "**Learning Experience:**\n",
        "Overall, working on extracting functions from textual content information become a precious studying enjoy. It supplied a realistic know-how of various strategies used in natural language processing (NLP) for feature extraction. the key standards i found most beneficial had been Bag-of-phrases (BoW), TF-IDF, N-grams, and function choice methods like mutual information. know-how how to constitute textual content data in a format appropriate for gadget mastering models is vital in NLP tasks, and this exercise helped support the ones ideas.\n",
        "\n",
        "**Challenges Encountered:**\n",
        "One task I encountered turned into setting up the surroundings and putting in the important libraries. additionally, knowledge the intricacies of characteristic choice techniques and making sure the proper preprocessing of text facts required some attempt. but, with the assist of documentation and on-line sources, i was capable to conquer these demanding situations.\n",
        "\n",
        "**Relevance to Your Field of Study:**\n",
        "This workout is enormously applicable to the sphere of NLP. function extraction is a essential step in text processing obligations such as sentiment analysis, text type, and records retrieval. by means of studying a way to extract and choose features from text facts, I gained insights into the preprocessing steps necessary for building powerful NLP models. This know-how is essential for everybody working in the subject of NLP, as it bureaucracy the muse for greater advanced strategies and algorithms.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}