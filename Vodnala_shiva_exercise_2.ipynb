{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vodnalashiva131/INFO-5731/blob/main/Vodnala_shiva_exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2vspm32Z5Er"
      },
      "source": [
        "Research Question: What are the factors that influence the success of a YouTube video?\n",
        "\n",
        "Data to Collect:\n",
        "*   Video title\n",
        "*   Video description\n",
        "*   Video category\n",
        "*   Video length\n",
        "*   Number of views\n",
        "*   Number of likes\n",
        "*   Number of dislikes\n",
        "*   Number of comments\n",
        "*   Video upload date\n",
        "*   Channel name\n",
        "*   Channel subscriber count\n",
        "\n",
        "Amount of Data Needed:\n",
        "*   We need at 1000 video for analysis\n",
        "\n",
        "Steps for Collecting and Saving the Data:\n",
        "1. Web Scraping:\n",
        "    *   To extract data from YouTube,we should use a web scraping tool like Beautiful Soup or Selenium.\n",
        "    *   We should create a script that opens the YouTube search page and types in the relevant query.\n",
        "    *   It is required to take what information is pertinent from the search results.\n",
        "2. Data Cleaning:\n",
        "    *   Remove any duplicate data.\n",
        "    *   Remove any missing data.\n",
        "    *   Format the extracted data into a consistent format were it is supported.\n",
        "3. Data Saving:\n",
        "    *   Cleaned data should in a CSV formated file.\n",
        "    *   Rename the file according to the requirment.\n",
        "    *   Best way to save data or prevent losing it should be storing in a safe location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "outputs": [],
      "source": [
        "# Seting libraries for chrome\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Seting up the chrome\n",
        "chrome = Options()\n",
        "chrome.add_argument('--headless')\n",
        "chrome.add_argument('--no-sandbox')\n",
        "chrome.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# intializing the chrome driver\n",
        "chrome_driver = webdriver.Chrome(options=chrome)\n",
        "\n",
        "# BASE URL\n",
        "base_url = \"https://www.youtube.com/results?search_query=\"\n",
        "\n",
        "# Declaring the Search Term\n",
        "search_term = \"youtube\"\n",
        "\n",
        "# Adding the Base URL and with the search term for generating the main URL\n",
        "search_url = base_url + search_term\n",
        "\n",
        "data = []\n",
        "\n",
        "for i in range(1000):\n",
        "    # Opening the search URL\n",
        "    chrome_driver.get(search_url)\n",
        "    time.sleep(5)\n",
        "    page_source = chrome_driver.page_source\n",
        "    soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "    # Extracting the Video Indetails from the search URL\n",
        "    video_titles = [a.text for a in soup.select(\"h3 > a\")]\n",
        "    video_urls = [a['href'] for a in soup.select(\"h3 > a\")]\n",
        "    views = [span.text for span in soup.select(\"span.view-count\")]\n",
        "    upload_dates = [span.text for span in soup.select(\"span.date\")]\n",
        "    channel_names = [a.text for a in soup.select(\"a.yt-user-name\")]\n",
        "\n",
        "    # Using the apend function to append the data\n",
        "    data.append({\n",
        "        \"title\": video_titles,\n",
        "        \"url\": video_urls,\n",
        "        \"views\": views,\n",
        "        \"upload_date\": upload_dates,\n",
        "        \"channel_name\": channel_names\n",
        "    })\n",
        "\n",
        "    # Extracting the next data from the next URL\n",
        "    next_page_url = soup.select_one(\"a[aria-label='Next page']\")\n",
        "    if next_page_url:\n",
        "        search_url = \"https://www.youtube.com\" + next_page_url['href']\n",
        "    else:\n",
        "        break\n",
        "\n",
        "    # Using this from avoid blocking us from the server\n",
        "    time.sleep(5)\n",
        "chrome_driver.quit()\n",
        "# Converting the data into a Pandas Frame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"youtube_data.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaGLbSHHB8Ej",
        "outputId": "28908b16-3ae7-4ed0-fe84-644e46f0b58f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [00:44<00:10, 10.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing paper: 'NoneType' object has no attribute 'find'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:56<00:00, 11.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                Title  \\\n",
            "0   The xyz algorithm for fast interaction search ...   \n",
            "1   Importance of Internalization of Tacit Knowled...   \n",
            "2   Data Governance to Improve Data Quality for St...   \n",
            "3   XYZ-Randomization using TSVs for Low-Latency E...   \n",
            "4   Implementation of The Fuzzy Inference System t...   \n",
            "..                                                ...   \n",
            "44  MadMax: surviving out-of-gas conditions in Eth...   \n",
            "45  A Machine Learning Approach for Detection Plan...   \n",
            "46  PLL to the rescue: a novel EM fault countermea...   \n",
            "47  Why GPUs are Slow at Executing NFAs and How to...   \n",
            "48  Analyzing the Keystroke Dynamics of Web Identi...   \n",
            "\n",
            "                                                 Link  \\\n",
            "0   https://dl.acm.org/doi/pdf/10.5555/3291125.329...   \n",
            "1   https://dl.acm.org/doi/pdf/10.1145/3429789.342...   \n",
            "2   https://dl.acm.org/doi/pdf/10.1145/3451471.345...   \n",
            "3   https://dl.acm.org/doi/pdf/10.1145/3130218.313...   \n",
            "4   https://dl.acm.org/doi/pdf/10.1145/3557738.355...   \n",
            "..                                                ...   \n",
            "44         https://dl.acm.org/doi/pdf/10.1145/3276486   \n",
            "45  https://dl.acm.org/doi/pdf/10.1145/3387168.338...   \n",
            "46  https://dl.acm.org/doi/pdf/10.1145/2897937.289...   \n",
            "47  https://dl.acm.org/doi/pdf/10.1145/3373376.337...   \n",
            "48  https://dl.acm.org/doi/pdf/10.1145/3091478.309...   \n",
            "\n",
            "                                             Abstract  \n",
            "0   When performing regression on a data set with ...  \n",
            "1   Tacit Knowledge is an essential part of the pr...  \n",
            "2   PT XYZ is having difficulty to implement a con...  \n",
            "3   In this paper, we propose a method to design l...  \n",
            "4   The Health Service Industry is one of the impo...  \n",
            "..                                                ...  \n",
            "44  Ethereum is a distributed blockchain platform,...  \n",
            "45  The export value of flower production is very ...  \n",
            "46  Electromagnetic injection (EMI) is a powerful ...  \n",
            "47  Non-deterministic Finite Automata (NFA) are sp...  \n",
            "48  Web identifiers such as usernames, hashtags, a...  \n",
            "\n",
            "[199 rows x 3 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "class ACMScraper(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def payload(self, keyword, st_page=0, pasize=50, start_year=2018, end_year=2022):\n",
        "        params = (\n",
        "            (\"AllField\", keyword),\n",
        "            (\"AfterYear\", str(start_year)),\n",
        "            (\"BeforeYear\", str(end_year)),\n",
        "            (\"queryID\", \"45/3852851837\"),\n",
        "            (\"sortBy\", \"relevancy\"),\n",
        "            (\"startPage\", str(st_page)),\n",
        "            (\"pageSize\", str(pasize)),\n",
        "        )\n",
        "\n",
        "        response = requests.get(\n",
        "            \"https://dl.acm.org/action/doSearch\",\n",
        "            params=params,\n",
        "            headers={\"accept\": \"application/json\"},\n",
        "        )\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        return soup\n",
        "\n",
        "    def soup_html(self, soup):\n",
        "        all_papers = []\n",
        "        main_class = soup.find(\n",
        "            \"div\", {\"class\": \"col-lg-9 col-md-9 col-sm-8 sticko__side-content\"}\n",
        "        )\n",
        "        main_c = main_class.find_all(\"div\", {\"class\": \"issue-item__content\"})\n",
        "\n",
        "        for paper in main_c:\n",
        "            temp_data = {}\n",
        "\n",
        "            try:\n",
        "                content_ = paper.find(\"h5\", {\"class\": \"issue-item__title\"})\n",
        "                paper_url = content_.find(\"a\", href=True)[\"href\"].split(\"/\")\n",
        "\n",
        "                title = content_.text\n",
        "                temp_data[\"Title\"] = title\n",
        "\n",
        "                doi_url = [\"https://dl.acm.org\", \"doi\", \"pdf\"]\n",
        "                doi_url.extend(paper_url[2:])\n",
        "                temp_data[\"Link\"] = \"/\".join(doi_url)\n",
        "\n",
        "                # Extract additional details\n",
        "                details = paper.find(\"div\", {\"class\": \"issue-item__detail\"})\n",
        "                venue = details.find(\"span\", {\"class\": \"issue-item__detail__text\"})\n",
        "                if venue:\n",
        "                    temp_data[\"Venue\"] = venue.text\n",
        "\n",
        "                year = details.find(\"span\", {\"class\": \"issue-item__detail__text\"})\n",
        "                if year:\n",
        "                    temp_data[\"Year\"] = year.text\n",
        "\n",
        "                authors = details.find(\"span\", {\"class\": \"loa__author-name\"})\n",
        "                if authors:\n",
        "                    temp_data[\"Authors\"] = authors.text\n",
        "\n",
        "                abstract = paper.find(\"div\", {\"class\": \"issue-item__abstract\"})\n",
        "                if abstract:\n",
        "                    temp_data[\"Abstract\"] = abstract.text.strip()\n",
        "\n",
        "                all_papers.append(temp_data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing paper: {e}\")\n",
        "\n",
        "        df = pd.DataFrame(all_papers)\n",
        "        return df\n",
        "\n",
        "    def acm(\n",
        "        self,\n",
        "        keyword,\n",
        "        max_pages=5,\n",
        "        min_year=2015,\n",
        "        max_year=2022,\n",
        "        full_page_result=False,\n",
        "        api_wait=5,\n",
        "    ):\n",
        "        all_pages = []\n",
        "\n",
        "        for page in tqdm(range(max_pages)):\n",
        "            acm_soup = self.payload(\n",
        "                keyword, st_page=page, pasize=50, start_year=min_year, end_year=max_year\n",
        "            )\n",
        "\n",
        "            acm_result = self.soup_html(acm_soup)\n",
        "            all_pages.append(acm_result)\n",
        "            time.sleep(api_wait)\n",
        "\n",
        "        df = pd.concat(all_pages)\n",
        "        return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"XYZ\"\n",
        "    acm_scraper = ACMScraper()\n",
        "    articles_df = acm_scraper.acm(keyword, max_pages=5)\n",
        "    print(articles_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDtLM6zIs1hW",
        "outputId": "db355584-25a4-46d8-a9f7-a3879fbf98ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -q instaloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCdCiXmPpVws",
        "outputId": "88205295-87f4-46b8-b993-916f36f3c2ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\win_unicode_console\\__init__.py:31: RuntimeWarning: sys.stdin.encoding == 'utf-8', whereas sys.stdout.encoding == 'UTF-8', readline hook consumer may assume they are the same\n",
            "  readline_hook.enable(use_pyreadline=use_pyreadline)\n",
            "JSON Query to explore/tags/gadgets/: 404 Not Found [retrying; skip with ^C]\n",
            "JSON Query to explore/tags/gadgets/: 404 Not Found [retrying; skip with ^C]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: JSON Query to explore/tags/gadgets/: 404 Not Found\n",
            "The following skipped Hashtag is: gadgets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "JSON Query to explore/tags/another_hashtag/: 404 Not Found [retrying; skip with ^C]\n",
            "JSON Query to explore/tags/another_hashtag/: 404 Not Found [retrying; skip with ^C]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: JSON Query to explore/tags/another_hashtag/: 404 Not Found\n",
            "The following skipped Hashtag is: another_hashtag\n"
          ]
        }
      ],
      "source": [
        "import instaloader\n",
        "from instaloader.exceptions import QueryReturnedNotFoundException\n",
        "from datetime import datetime\n",
        "import csv\n",
        "\n",
        "class GetInstagramProfile():\n",
        "    def __init__(self):\n",
        "        self.L = instaloader.Instaloader()\n",
        "\n",
        "    def download_users_profile_picture(self, username):\n",
        "        self.L.download_profile(username, profile_pic_only=True)\n",
        "\n",
        "    def download_hashtag_posts(self, hashtag):\n",
        "        try:\n",
        "            for post in instaloader.Hashtag.from_name(self.L.context, hashtag).get_posts():\n",
        "                self.L.download_post(post, target='#' + hashtag)\n",
        "        except QueryReturnedNotFoundException as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            print(f\"The following skipped Hashtag is: {hashtag}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cls = GetInstagramProfile()\n",
        "    hashtags = [\"gadgets\", \"another_hashtag\"]\n",
        "\n",
        "    for hashtag in hashtags:\n",
        "        cls.download_hashtag_posts(hashtag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Questionitalicized text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "Learning Experience:\n",
        "The web scraping assignments gave participants practical experience obtaining data from internet sources. Understanding HTML structure, processing HTTP requests, and parsing HTML using BeautifulSoup were among the most important lessons learned. Developing critical abilities such as navigating and adjusting to various website architectures was necessary.\n",
        "\n",
        "Challenges Encountered while performing this exercise:\n",
        "There were difficulties with dynamic material loaded via JavaScript. Alternative data sources or headless browsers were required as solutions. Ethical scraping required careful attention to rate restrictions, avoiding IP blocking, and observing terms of service on websites.\n",
        "\n",
        "similar Topic to my Field of Study:\n",
        "It is useful in many different sectors to collect and analyse data from internet sources. Web scraping offers research and decision-making insights in data science, social sciences, and business analytics. Decisions are made with more knowledge and based on data thanks to the improved data collecting efficiency brought about by the learned abilities."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "07bf2e0fcd790eb3c900db4935f9ba6ea43c1c1bb69b00af6a6283ca1c5858b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}